from typing import List, Any, Dict, Optional
from fastapi import APIRouter, HTTPException, status, Depends, BackgroundTasks, Query
from sqlmodel import select, desc, func
from sqlalchemy import tuple_
from datetime import date, timedelta, datetime
import json
from pydantic import BaseModel

from app.api.deps import CurrentUser, DbSession
from app.models.competitor import Competitor, CompetitorRate, CompetitorSource
from app.models.hotel import Hotel
from app.models.room import RoomType
from app.models.rates import RoomRate
from app.core.redis_client import redis_client
from app.core.database import async_session
from app.schemas.rate_ingest import RateIngestRequest

router = APIRouter(prefix="/competitors", tags=["Competitor Rates"])

@router.get("", response_model=List[Competitor])
async def list_competitors(current_user: CurrentUser, session: DbSession):
    """List all competitors for current hotel"""
    query = select(Competitor).where(Competitor.hotel_id == current_user.hotel_id)
    result = await session.execute(query)
    return result.scalars().all()

@router.post("", response_model=Competitor)
async def add_competitor(comp_data: Competitor, current_user: CurrentUser, session: DbSession, background_tasks: BackgroundTasks):
    """Add a new competitor to track"""
    # Force hotel_id
    comp_data.hotel_id = current_user.hotel_id
    
    # Check for duplicates (URL or Name)
    existing = await session.execute(
        select(Competitor).where(
            Competitor.hotel_id == current_user.hotel_id,
            (Competitor.url == comp_data.url) | (Competitor.name == comp_data.name)
        )
    )
    existing_comp = existing.scalars().first()
    
    if existing_comp:
        return existing_comp

    session.add(comp_data)
    await session.commit()
    await session.refresh(comp_data)
    
    return comp_data

@router.post("/{comp_id}/scrape")
async def trigger_scrape(comp_id: str, current_user: CurrentUser, session: DbSession, background_tasks: BackgroundTasks):
    """Manually trigger a scrape"""
    background_tasks.add_task(run_background_scrape, comp_id)
    return {"message": "Scrape started in background"}

async def run_background_scrape(comp_id: str):
    """Wrapper to run scrape in its own DB session"""
    print(f"Starting Background Scrape for {comp_id}")
    try:
        # Placeholder for actual scraping logic if needed in future
        pass
    except Exception as e:
        print(f"Background Scrape CRASHED for {comp_id}: {e}")

@router.delete("/{comp_id}")
async def delete_competitor(comp_id: str, current_user: CurrentUser, session: DbSession):
    """Delete a competitor and all their rate history"""
    comp = await session.get(Competitor, comp_id)
    if not comp:
        raise HTTPException(status_code=404, detail="Competitor not found")
        
    if comp.hotel_id != current_user.hotel_id:
        raise HTTPException(status_code=403, detail="Not authorized")
        
    # Delete rates explicitly first (if cascade not set, safe bet)
    rates_stmt = select(CompetitorRate).where(CompetitorRate.competitor_id == comp_id)
    rates_res = await session.execute(rates_stmt)
    rates = rates_res.scalars().all()
    for r in rates:
        await session.delete(r)
        
    await session.delete(comp)
    await session.commit()
    
    return {"message": "Competitor deleted successfully"}

# --- Market Analysis ---

class MarketAnalysisResult(BaseModel):
    date: str
    my_price: float
    lowest_market_price: float
    average_market_price: float
    highest_market_price: float
    market_position: str # "Premium", "Budget", "Average"
    suggestion: str

@router.get("/analysis", response_model=List[MarketAnalysisResult])
async def get_market_analysis(
    current_user: CurrentUser,
    session: DbSession,
    days: int = 7,
    start_date: date = None
):
    """
    Analyzes market rates for the next N days.
    Optimized: 15s -> <500ms using Redis + Efficient Queries
    """
    today = start_date if start_date else date.today()

    # 1. Check Redis Cache (Market Analysis is heavy, cache for 1 hour)
    cache_key = f"market_analysis:{current_user.hotel_id}:{today.isoformat()}"
    try:
        r = redis_client.get_instance()
        cached = r.get(cache_key)
        if cached:
            return json.loads(cached)
    except: pass

    end_date = today + timedelta(days=days)

    # 1. Fetch My Rates (First Room Type)
    rt_query = select(RoomType).where(RoomType.hotel_id == current_user.hotel_id)
    rt_res = await session.execute(rt_query)
    room_type = rt_res.scalars().first()

    if not room_type:
        return []

    # Get my rates map
    my_rates_map = {}
    base_price = room_type.base_price
    # Default to base price
    for i in range(days):
        d = today + timedelta(days=i)
        my_rates_map[d] = base_price

    # 2. Fetch Competitor Rates efficiently (Use Index!)
    # Instead of fetching ALL rates and filtering in Python, fetch only needed range
    # And crucially, we need the LATEST fetch only.

    comp_ids_result = await session.execute(select(Competitor.id).where(Competitor.hotel_id == current_user.hotel_id))
    comp_ids = comp_ids_result.scalars().all()
    
    rates_by_date = {}

    if comp_ids:
        # Optimized Query: Fetch only rates in range, ordered by fetch time DESC
        # We rely on Python to pick the first one (latest) per (comp, date) pair
        # This is faster than complex SQL subqueries for small N (days=7)
        rate_query = select(CompetitorRate).where(
            CompetitorRate.competitor_id.in_(comp_ids),
            CompetitorRate.check_in_date >= today,
            CompetitorRate.check_in_date < end_date
        ).order_by(
            CompetitorRate.competitor_id,
            CompetitorRate.check_in_date,
            desc(CompetitorRate.fetched_at)
        )

        rates_res = await session.execute(rate_query)
        all_rates = rates_res.scalars().all()

        # Group by date - Keeping only the FIRST encountered (which is Latest due to DESC sort)
        seen_keys = set()

        for r in all_rates:
            key = (r.competitor_id, r.check_in_date)
            if key not in seen_keys:
                seen_keys.add(key)
                if r.check_in_date not in rates_by_date:
                    rates_by_date[r.check_in_date] = []
                rates_by_date[r.check_in_date].append(r.price)

    # 3. Analyze
    results = []
    for i in range(days):
        d = today + timedelta(days=i)
        my_price = my_rates_map.get(d, 0)
        market_prices = rates_by_date.get(d, [])

        if not market_prices:
            # No data
            results.append({
                "date": d.isoformat(),
                "my_price": my_price,
                "lowest_market_price": 0,
                "average_market_price": 0,
                "highest_market_price": 0,
                "market_position": "Unknown",
                "suggestion": "No competitor data available. Check Chrome Extension."
            })
            continue

        lowest = min(market_prices)
        highest = max(market_prices)
        avg = sum(market_prices) / len(market_prices)

        # Position Logic
        if my_price > avg * 1.1:
            position = "Premium"
            suggestion = "Price is significantly higher than market average. Consider lowering if occupancy is low."
        elif my_price < avg * 0.9:
            position = "Budget"
            suggestion = "Price is lower than market. Opportunity to increase rate."
        else:
            position = "Average"
            suggestion = "Price is competitive with market average."

        results.append({
            "date": d.isoformat(),
            "my_price": my_price,
            "lowest_market_price": lowest,
            "average_market_price": int(avg),
            "highest_market_price": highest,
            "market_position": position,
            "suggestion": suggestion
        })

    # Cache for 1 Hour
    try:
        r.setex(cache_key, 3600, json.dumps(results))
    except: pass

    return results


@router.get("/rates/comparison")
async def get_rate_comparison(current_user: CurrentUser, session: DbSession, start_date: date = None):
    """
    Get data for chart: My Rate vs Competitors for next 7 days.
    """
    today = start_date if start_date else date.today()
    end_date = today + timedelta(days=7)
    
    # Cache Check
    cache_key = f"rate_comparison:{current_user.hotel_id}:{today.isoformat()}"
    try:
        r = redis_client.get_instance()
        cached = r.get(cache_key)
        if cached:
            return json.loads(cached)
    except: pass

    # 1. Fetch My Rates
    rt_query = select(RoomType).where(RoomType.hotel_id == current_user.hotel_id)
    rt_res = await session.execute(rt_query)
    room_type = rt_res.scalars().first()
    
    my_rates_map = {}
    if room_type:
        base_price = room_type.base_price
        for i in range(7):
            d = today + timedelta(days=i)
            my_rates_map[d] = base_price
            
    # 2. Fetch Competitor Rates
    comp_query = select(Competitor).where(Competitor.hotel_id == current_user.hotel_id)
    comp_res = await session.execute(comp_query)
    competitors = comp_res.scalars().all()
    
    # 3. Bulk Fetch Competitor Rates (Optimization)
    comp_ids = [c.id for c in competitors]
    rates_map = {} # (competitor_id, check_in_date) -> RateObj
    
    if comp_ids:
        # Fetch all rates for these competitors in the date range in ONE query
        rate_query = select(CompetitorRate).where(
            CompetitorRate.competitor_id.in_(comp_ids),
            CompetitorRate.check_in_date >= today,
            CompetitorRate.check_in_date < end_date
        ).order_by(desc(CompetitorRate.fetched_at)) # Latest first

        rate_res = await session.execute(rate_query)
        all_rates = rate_res.scalars().all()

        # Populate map (since ordered by fetched_at desc, first encounter is latest)
        for r in all_rates:
            key = (r.competitor_id, r.check_in_date)
            if key not in rates_map:
                rates_map[key] = r

    # 4. Build Response Data (Iterate 7 days)
    chart_data = [] 
    table_data = []
    
    for i in range(7):
        d = today + timedelta(days=i)
        date_str = d.strftime("%d %b")
        
        # Initialize Day Chart
        day_chart = {
            "date": date_str,
            "My Hotel": my_rates_map.get(d, 0)
        }
        
        # Initialize Day Table
        day_table = {
            "date": date_str,
            "full_date": d.isoformat(),
            "my_rate": {
                "price": my_rates_map.get(d, 0),
                "room_type": room_type.name if room_type else "Standard" 
            },
            "competitors": {}
        }
        
        # Fill Competitor Data for this day
        for comp in competitors:
            # O(1) Lookup from memory
            rate = rates_map.get((comp.id, d))
            
            if rate:
                day_chart[comp.name] = rate.price
                day_table["competitors"][comp.name] = {
                    "price": rate.price,
                    "room_type": rate.room_type,
                    "is_sold_out": rate.is_sold_out,
                    "source": comp.source,
                    "url": comp.url
                }
        
        # Append to Main Lists
        chart_data.append(day_chart)
        table_data.append(day_table)
        
    final_res = {
        "chart_data": chart_data,
        "table_data": table_data,
        "competitors": [c.name for c in competitors]
    }

    # Cache for 1 Hour
    try:
        r.setex(cache_key, 3600, json.dumps(final_res))
    except: pass

    return final_res

@router.post("/rates/ingest", response_model=dict)
async def ingest_competitor_rates(
    payload: RateIngestRequest,
    session: DbSession,
    current_user: CurrentUser
):
    """
    Ingest rates from Chrome Extension (Authenticated).
    """
    if not payload.rates:
        return {"message": "No rates provided", "status": "warning"}

    comp_ids = {item.competitor_id for item in payload.rates}
    
    comp_query = select(Competitor.id).where(
        Competitor.id.in_(comp_ids),
        Competitor.hotel_id == current_user.hotel_id
    )
    valid_comp_ids = (await session.execute(comp_query)).scalars().all()
    valid_comp_ids = set(valid_comp_ids)

    if not valid_comp_ids:
        return {"message": "No valid competitors found for this user", "status": "warning"}

    valid_rates_payload = [r for r in payload.rates if r.competitor_id in valid_comp_ids]

    if not valid_rates_payload:
         return {"message": "No valid rates to ingest", "status": "warning"}

    keys = [(r.competitor_id, r.check_in_date) for r in valid_rates_payload]

    existing_rates_query = select(CompetitorRate).where(
        tuple_(CompetitorRate.competitor_id, CompetitorRate.check_in_date).in_(keys)
    )
    existing_rates_result = await session.execute(existing_rates_query)
    existing_rates = existing_rates_result.scalars().all()

    existing_map = {(r.competitor_id, r.check_in_date): r for r in existing_rates}

    count_new = 0
    count_update = 0

    for item in valid_rates_payload:
        key = (item.competitor_id, item.check_in_date)
        
        if key in existing_map:
            rate_obj = existing_map[key]
            rate_obj.price = item.price
            rate_obj.is_sold_out = item.is_sold_out
            rate_obj.room_type = item.room_type
            rate_obj.fetched_at = datetime.utcnow()
            session.add(rate_obj)
            count_update += 1
        else:
            new_rate = CompetitorRate(
                competitor_id=item.competitor_id,
                check_in_date=item.check_in_date,
                price=item.price,
                is_sold_out=item.is_sold_out,
                room_type=item.room_type,
                currency=item.currency,
                fetched_at=datetime.utcnow()
            )
            session.add(new_rate)
            count_new += 1
            
    await session.commit()

    # --- Redis Write-Through (Performance) ---
    try:
        r = redis_client.get_instance()
        pipe = r.pipeline()
        for item in valid_rates_payload:
            key = f"rate:{item.competitor_id}:{item.check_in_date.isoformat()}"
            pipe.setex(key, 86400, "1") # 24h Expiry

            # Invalidate Market Analysis Cache immediately
            # Because rate changed, analysis might change
            cache_key_analysis = f"market_analysis:{current_user.hotel_id}:{item.check_in_date.isoformat()}"
            pipe.delete(cache_key_analysis)

        pipe.execute()
    except Exception as e:
         print(f"Redis Write Failed (Ignored): {e}")

    return {
        "message": f"Processed {len(valid_rates_payload)} rates (New: {count_new}, Updated: {count_update})",
        "status": "success"
    }

class ScrapeJobItem(BaseModel):
    competitor_id: str
    check_in_date: date

class CheckFreshnessResponse(BaseModel):
    jobs_to_scrape: List[ScrapeJobItem]
    cached_count: int

@router.post("/check_freshness", response_model=CheckFreshnessResponse)
async def check_scrape_freshness(jobs: List[ScrapeJobItem], session: DbSession):
    """
    Check if rates already exist in PostgreSQL (Supabase) instead of Redis.
    """
    to_scrape = []
    cached_hits = 0
    
    # 1. Fast Path: Check Redis
    redis_misses = [] # List of jobs not found in Redis
    try:
        r = redis_client.get_instance()
        pipe = r.pipeline()
        for job in jobs:
            key = f"rate:{job.competitor_id}:{job.check_in_date.isoformat()}"
            pipe.exists(key)
        results = pipe.execute()
        
        for i, exists in enumerate(results):
            if exists:
                cached_hits += 1
            else:
                redis_misses.append(jobs[i])
    except Exception as e:
        print(f"Redis Check Failed: {e}")
        redis_misses = jobs # Fallback to DB check for all if Redis fails
    
    if not redis_misses:
        return {"jobs_to_scrape": [], "cached_count": cached_hits}

    # 2. Slow Path: Check DB for Redis Misses
    comp_ids = {job.competitor_id for job in redis_misses}
    dates = {job.check_in_date for job in redis_misses}
    
    query = select(CompetitorRate.competitor_id, CompetitorRate.check_in_date).where(
        CompetitorRate.competitor_id.in_(comp_ids),
        CompetitorRate.check_in_date.in_(dates),
        CompetitorRate.fetched_at >= datetime.utcnow() - timedelta(hours=24)
    )
    
    res = await session.execute(query)
    existing = set(res.all())
    
    # 3. Populate Redis for DB Hits (Read-Repair)
    if existing:
        try:
            r = redis_client.get_instance()
            pipe = r.pipeline()
            for cid, cdate in existing:
                key = f"rate:{cid}:{cdate.isoformat()}"
                pipe.setex(key, 86400, "1")
            pipe.execute()
        except: pass

    for job in redis_misses:
        if (job.competitor_id, job.check_in_date) in existing:
            cached_hits += 1
        else:
            to_scrape.append(job)
                
    return {"jobs_to_scrape": to_scrape, "cached_count": cached_hits}
